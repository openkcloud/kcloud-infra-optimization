# Infrastructure Module - Cluster Management

**ë©€í‹° í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ ëª¨ë“ˆ**

## ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥

### ğŸŒŠ Magnum í´ëŸ¬ìŠ¤í„° ê´€ë¦¬
- **í´ëŸ¬ìŠ¤í„° ìƒì„±/ì‚­ì œ**: AI ê°€ì†ê¸°ë³„ ì „ìš© í´ëŸ¬ìŠ¤í„° ë™ì  ìƒì„±
- **ìŠ¤ì¼€ì¼ë§**: ì›Œí¬ë¡œë“œ ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ ë…¸ë“œ í™•ì¥/ì¶•ì†Œ
- **í´ëŸ¬ìŠ¤í„° í…œí”Œë¦¿**: GPU/NPU/CPU ì „ìš© í´ëŸ¬ìŠ¤í„° í…œí”Œë¦¿ ê´€ë¦¬
- **ìƒíƒœ ëª¨ë‹ˆí„°ë§**: í´ëŸ¬ìŠ¤í„° ìƒíƒœ ì¶”ì  ë° í—¬ìŠ¤ ì²´í¬

### ğŸ”§ Heat í…œí”Œë¦¿ ê´€ë¦¬
- **ì¸í”„ë¼ ì½”ë“œí™”**: Heat í…œí”Œë¦¿ì„ í†µí•œ ì¬í˜„ ê°€ëŠ¥í•œ ì¸í”„ë¼ êµ¬ì„±
- **ìì› ì •ì˜**: GPU/NPU ë…¸ë“œ íƒ€ì…ë³„ Heat ìŠ¤íƒ ê´€ë¦¬
- **ë„¤íŠ¸ì›Œí‚¹**: í´ëŸ¬ìŠ¤í„° ê°„ ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬ ë° ì—°ê²°

### âš¡ ì›Œí¬ë¡œë“œ ê¸°ë°˜ ìµœì í™”
- **í´ëŸ¬ìŠ¤í„° ë§¤ì¹­**: ì›Œí¬ë¡œë“œ íŠ¹ì„±ì— ë§ëŠ” ìµœì  í´ëŸ¬ìŠ¤í„° ì„ íƒ
- **ìì› í• ë‹¹**: ìœ íœ´ ìì› ìµœì†Œí™”ë¥¼ ìœ„í•œ í´ëŸ¬ìŠ¤í„° í†µí•©/ë¶„í• 
- **ë¹„ìš© ìµœì í™”**: ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í´ëŸ¬ìŠ¤í„° ìë™ ì •ë¦¬

## ğŸ— ì•„í‚¤í…ì²˜

```
kcloud-optimizer â†’ infrastructure â†’ OpenStack Magnum API
    â†“                     â†“              â†“
ì›Œí¬ë¡œë“œ ìš”êµ¬ì‚¬í•­    í´ëŸ¬ìŠ¤í„° ê²°ì •    ì‹¤ì œ í´ëŸ¬ìŠ¤í„° ìƒì„±/ê´€ë¦¬
    â†“                     â†“              â†“
core ìŠ¤ì¼€ì¤„ëŸ¬ â† Heat Templates â† Nova/Neutron/Cinder
```

## ğŸ¯ í´ëŸ¬ìŠ¤í„° íƒ€ì…ë³„ ì „ëµ

### **GPU-Intensive í´ëŸ¬ìŠ¤í„°**
- **êµ¬ì„±**: NVIDIA GPU ë…¸ë“œ + ê³ ì„±ëŠ¥ CPU
- **ìš©ë„**: ML í›ˆë ¨, ë”¥ëŸ¬ë‹ ëª¨ë¸ ê°œë°œ
- **ìµœì í™”**: ì „ë ¥ íš¨ìœ¨ì„± + GPU í™œìš©ë¥ 

### **NPU-Optimized í´ëŸ¬ìŠ¤í„°**  
- **êµ¬ì„±**: Intel/AMD NPU ë…¸ë“œ + ì €ì „ë ¥ CPU
- **ìš©ë„**: AI ì¶”ë¡ , ì‹¤ì‹œê°„ ì²˜ë¦¬
- **ìµœì í™”**: ì‘ë‹µ ì‹œê°„ + ì²˜ë¦¬ëŸ‰

### **Hybrid-Balanced í´ëŸ¬ìŠ¤í„°**
- **êµ¬ì„±**: GPU + NPU + CPU í˜¼í•©
- **ìš©ë„**: ë³µí•© AI ì›Œí¬ë¡œë“œ
- **ìµœì í™”**: ìì› í™œìš©ë¥  + ìœ ì—°ì„±

### **CPU-Only í´ëŸ¬ìŠ¤í„°**
- **êµ¬ì„±**: CPU ì „ìš©
- **ìš©ë„**: ì¼ë°˜ ì„œë¹„ìŠ¤, ë°ì´í„° ì²˜ë¦¬
- **ìµœì í™”**: ë¹„ìš© ìµœì†Œí™”

## ğŸš€ í•µì‹¬ ê¸°ëŠ¥

### Magnum í´ëŸ¬ìŠ¤í„° ìƒì„±
```python
# í´ëŸ¬ìŠ¤í„° ìƒì„± ì˜ˆì‹œ
cluster_config = {
    "name": "ml-training-cluster",
    "cluster_template_id": "gpu-intensive-template",
    "node_count": 4,
    "master_count": 1,
    "keypair": "kcloud-keypair",
    "labels": {
        "workload_type": "training",
        "gpu_type": "nvidia-a100",
        "power_optimization": "enabled"
    }
}

cluster = await magnum_client.create_cluster(cluster_config)
```

### ë™ì  ìŠ¤ì¼€ì¼ë§
```python
# ì›Œí¬ë¡œë“œ ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ ìŠ¤ì¼€ì¼ë§
scaling_decision = await optimizer.analyze_workload_requirements(workloads)

if scaling_decision.action == "scale_out":
    await magnum_client.resize_cluster(
        cluster_id=cluster.id,
        node_count=scaling_decision.target_nodes
    )
```

### Heat í…œí”Œë¦¿ ê´€ë¦¬
```yaml
# GPU ì „ìš© í´ëŸ¬ìŠ¤í„° í…œí”Œë¦¿ ì˜ˆì‹œ
heat_template_version: wallaby

parameters:
  gpu_flavor: {type: string, default: "gpu.a100.large"}
  gpu_count: {type: number, default: 4}
  network_id: {type: string}

resources:
  gpu_cluster:
    type: OS::Magnum::ClusterTemplate
    properties:
      name: gpu-intensive-template
      coe: kubernetes
      flavor_id: {get_param: gpu_flavor}
      master_flavor_id: "cpu.large"
      volume_driver: cinder
      network_driver: flannel
      labels:
        - "gpu_enabled=true"
        - "power_monitoring=kepler"
```

## ğŸ”§ ì„¤ì •

### OpenStack ì—°ë™ ì„¤ì •
```bash
# OpenStack ì¸ì¦
export OS_AUTH_URL=http://controller:5000/v3
export OS_PROJECT_NAME=kcloud
export OS_USERNAME=admin
export OS_PASSWORD=secretpassword
export OS_REGION_NAME=RegionOne
export OS_IDENTITY_API_VERSION=3

# Magnum ì„¤ì •
export MAGNUM_API_VERSION=1.15
export MAGNUM_ENDPOINT_TYPE=public

# Heat ì„¤ì •
export HEAT_API_VERSION=1
```

### í´ëŸ¬ìŠ¤í„° í…œí”Œë¦¿ ì„¤ì •
```yaml
cluster_templates:
  gpu_intensive:
    name: "gpu-intensive-template"
    coe: "kubernetes"
    image: "fedora-atomic-k8s"
    flavor: "gpu.a100.large"
    master_flavor: "cpu.large"
    node_count: 4
    master_count: 1
    volume_driver: "cinder"
    network_driver: "flannel"
    labels:
      gpu_enabled: "true"
      power_monitoring: "kepler"
      workload_type: "training"

  npu_optimized:
    name: "npu-optimized-template"
    coe: "kubernetes"
    image: "fedora-atomic-k8s"
    flavor: "npu.intel.medium"
    master_flavor: "cpu.medium"
    node_count: 2
    master_count: 1
    labels:
      npu_enabled: "true"
      workload_type: "inference"
```

## ğŸ“Š API ì—”ë“œí¬ì¸íŠ¸

```bash
# í´ëŸ¬ìŠ¤í„° ê´€ë¦¬
POST /clusters                    # í´ëŸ¬ìŠ¤í„° ìƒì„±
GET /clusters                     # í´ëŸ¬ìŠ¤í„° ëª©ë¡
GET /clusters/{cluster_id}        # í´ëŸ¬ìŠ¤í„° ìƒì„¸
PUT /clusters/{cluster_id}/scale  # í´ëŸ¬ìŠ¤í„° ìŠ¤ì¼€ì¼ë§
DELETE /clusters/{cluster_id}     # í´ëŸ¬ìŠ¤í„° ì‚­ì œ

# í´ëŸ¬ìŠ¤í„° í…œí”Œë¦¿ ê´€ë¦¬  
GET /templates                    # í…œí”Œë¦¿ ëª©ë¡
POST /templates                   # í…œí”Œë¦¿ ìƒì„±
GET /templates/{template_id}      # í…œí”Œë¦¿ ìƒì„¸

# ì›Œí¬ë¡œë“œ ë§¤ì¹­
POST /match/workload              # ì›Œí¬ë¡œë“œì— ìµœì  í´ëŸ¬ìŠ¤í„° ì¶”ì²œ
GET /clusters/available           # ì‚¬ìš© ê°€ëŠ¥í•œ í´ëŸ¬ìŠ¤í„° ëª©ë¡

# ëª¨ë‹ˆí„°ë§
GET /clusters/{cluster_id}/status # í´ëŸ¬ìŠ¤í„° ìƒíƒœ
GET /clusters/{cluster_id}/metrics # í´ëŸ¬ìŠ¤í„° ë©”íŠ¸ë¦­
GET /clusters/{cluster_id}/costs  # í´ëŸ¬ìŠ¤í„° ë¹„ìš©
```

## ğŸ§ª ì‚¬ìš© ì˜ˆì‹œ

```python
from infrastructure.magnum_client import MagnumClient
from infrastructure.cluster_manager import ClusterManager

# Magnum í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
magnum = MagnumClient(
    auth_url="http://controller:5000/v3",
    project_name="kcloud",
    username="admin",
    password="secretpassword"
)

# í´ëŸ¬ìŠ¤í„° ë§¤ë‹ˆì € ì´ˆê¸°í™”
manager = ClusterManager(magnum_client=magnum)

# ì›Œí¬ë¡œë“œ ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ìƒì„±
workload_requirements = {
    "type": "ml_training",
    "gpu_required": True,
    "gpu_count": 4,
    "cpu_cores": 32,
    "memory_gb": 128,
    "power_budget": 2000  # watts
}

# ìµœì  í´ëŸ¬ìŠ¤í„° ì¶”ì²œ ë° ìƒì„±
cluster = await manager.create_optimal_cluster(workload_requirements)
print(f"í´ëŸ¬ìŠ¤í„° ìƒì„± ì™„ë£Œ: {cluster.name} ({cluster.id})")

# ì›Œí¬ë¡œë“œ ë°°í¬ í›„ ëª¨ë‹ˆí„°ë§
status = await manager.get_cluster_status(cluster.id)
print(f"í´ëŸ¬ìŠ¤í„° ìƒíƒœ: {status.phase}, ë…¸ë“œ: {status.node_count}")
```

## ğŸš€ ë°°í¬

```bash
# ë¡œì»¬ ê°œë°œ
make install
make test
make run

# OpenStack í™˜ê²½ ì„¤ì • í™•ì¸
make verify-openstack

# Docker ì‹¤í–‰  
make docker-build
make docker-run

# K8s ë°°í¬
kubectl apply -f deployment/infrastructure.yaml
```

## ğŸ”’ ë³´ì•ˆ

- **OpenStack ì¸ì¦**: Keystoneì„ í†µí•œ ì•ˆì „í•œ API ì¸ì¦
- **RBAC**: í´ëŸ¬ìŠ¤í„°ë³„ ì—­í•  ê¸°ë°˜ ì ‘ê·¼ ì œì–´
- **ë„¤íŠ¸ì›Œí¬ ê²©ë¦¬**: Neutronì„ í†µí•œ í´ëŸ¬ìŠ¤í„° ê°„ ë„¤íŠ¸ì›Œí¬ ë¶„ë¦¬
- **ì‹œí¬ë¦¿ ê´€ë¦¬**: í´ëŸ¬ìŠ¤í„° ì¸ì¦ì„œ ë° í‚¤ ë³´ì•ˆ ì €ì¥
